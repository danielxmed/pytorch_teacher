# Handoff: Enriquecimento dos Pr√≥ximos M√≥dulos

Este documento serve como guia para agentes futuros que continuar√£o o trabalho de enriquecimento do conte√∫do da PyTorch Academy.

## Status Atual

### ‚úÖ Completo: Se√ß√£o 1 - Fundamentos (M√≥dulos 1-4)

| M√≥dulo | Arquivo | Linhas Antes | Linhas Depois | Exerc√≠cios |
|--------|---------|--------------|---------------|------------|
| 01-tensors | lesson.mdx | ~206 | ~617 | 4 ‚Üí 7 |
| 02-tensor-operations | lesson.mdx | ~255 | ~785 | 4 ‚Üí 6 |
| 03-shape-manipulation | lesson.mdx | ~255 | ~710 | 4 ‚Üí 6 |
| 04-tensors-numpy | lesson.mdx | ~260 | ~567 | 3 ‚Üí 5 |

**Padr√µes aplicados:**
- Se√ß√£o "Por que..." explicando motiva√ß√£o e contexto
- Mais CodeCells com exemplos pr√°ticos e comentados
- Tabelas de refer√™ncia r√°pida
- Callouts com dicas, warnings e informa√ß√µes importantes
- Exerc√≠cios de diferentes dificuldades (easy, medium, hard)
- Refer√™ncias a aplica√ß√µes em deep learning

---

### ‚úÖ Completo: Se√ß√£o 2 - Autograd (M√≥dulos 5-7)

| M√≥dulo | Arquivo | Linhas Antes | Linhas Depois | Exerc√≠cios |
|--------|---------|--------------|---------------|------------|
| 05-autograd-intro | lesson.mdx | ~254 | ~777 | 3 ‚Üí 7 |
| 06-computational-graph | lesson.mdx | ~210 | ~783 | 2 ‚Üí 6 |
| 07-gradients-practice | lesson.mdx | ~214 | ~985 | 2 ‚Üí 7 |

**Conte√∫do expandido:**
- **M√≥dulo 05**: Se√ß√µes sobre "Por que Autograd?", atributo `.grad`, leaf vs non-leaf tensors, padr√µes de treinamento
- **M√≥dulo 06**: Navega√ß√£o de `grad_fn`, `next_functions`, `retain_grad()`, `detach()`, grafos din√¢micos
- **M√≥dulo 07**: `torch.autograd.grad()`, gradient clipping, debugging de gradientes, gradientes de ordem superior, training loop completo

**Exerc√≠cios adicionados:**
- C√°lculo de gradientes simples e compostos
- Acumula√ß√£o e zeragem de gradientes
- Training steps com gradient descent
- Congelamento de par√¢metros
- Uso de `retain_grad()` e `detach()`
- Navega√ß√£o manual do grafo computacional
- Gradient clipping e segunda derivada

---

## üìã Pr√≥ximos Blocos para Enriquecimento

### ‚úÖ Bloco 2: Autograd (M√≥dulos 5-7) - COMPLETO

Estes m√≥dulos s√£o fundamentais para entender como o PyTorch treina redes neurais.

- [x] **05-autograd-intro** (`content/05-autograd-intro/`)
  - ‚úÖ Expandir explica√ß√£o de diferencia√ß√£o autom√°tica
  - ‚úÖ Adicionar visualiza√ß√£o do grafo computacional
  - ‚úÖ Exemplos pr√°ticos de gradientes simples
  - ‚úÖ Compara√ß√£o com c√°lculo manual de derivadas

- [x] **06-computational-graph** (`content/06-computational-graph/`)
  - ‚úÖ Detalhes de como o grafo √© constru√≠do
  - ‚úÖ Opera√ß√µes leaf vs non-leaf
  - ‚úÖ `retain_grad()` e quando usar
  - ‚úÖ Visualiza√ß√£o de grafos com diagramas conceituais

- [x] **07-gradients-practice** (`content/07-gradients-practice/`)
  - ‚úÖ Exerc√≠cios pr√°ticos de backpropagation
  - ‚úÖ `backward()` com argumentos
  - ‚úÖ `torch.no_grad()` e `torch.inference_mode()`
  - ‚úÖ Debugging de gradientes (NaN, explos√£o, etc.)

**Se√ß√µes da documenta√ß√£o utilizadas:**
- Se√ß√£o sobre autograd no `docs_pytorch/docs.pytorch.org-llms.md`
- Refer√™ncias: "autograd", "backward", "gradient", "requires_grad"

---

### Bloco 3: Redes Neurais (M√≥dulos 8-12) - PRIORIDADE ALTA

Core do PyTorch para constru√ß√£o de modelos.

- [ ] **08-nn-module** (`content/08-nn-module/`)
  - `nn.Module` em profundidade
  - `forward()` e `__call__`
  - Registro de par√¢metros
  - Submodules e composi√ß√£o

- [ ] **09-builtin-layers** (`content/09-builtin-layers/`)
  - Linear, Conv2d, BatchNorm, Dropout
  - Par√¢metros de cada camada
  - Quando usar cada tipo

- [ ] **10-activations-loss** (`content/10-activations-loss/`)
  - ReLU, Sigmoid, Tanh, Softmax, GELU
  - CrossEntropyLoss, MSELoss, BCELoss
  - Escolha de fun√ß√£o de perda por problema

- [ ] **11-optimizers** (`content/11-optimizers/`)
  - SGD, Adam, AdamW, RMSprop
  - Learning rate scheduling
  - Weight decay e regulariza√ß√£o

- [ ] **12-training-loop** (`content/12-training-loop/`)
  - Loop de treinamento completo
  - Valida√ß√£o e early stopping
  - Checkpointing de modelos

**Se√ß√µes da documenta√ß√£o relevantes:**
- `torch.nn` module
- `torch.optim` module
- Buscar: "nn.Module", "Linear", "Conv2d", "optimizer"

---

### Bloco 4: Dados e Treinamento (M√≥dulos 13-15) - PRIORIDADE M√âDIA

Essencial para trabalhar com datasets reais.

- [ ] **13-dataset-dataloader** (`content/13-dataset-dataloader/`)
  - `Dataset` e `IterableDataset`
  - `DataLoader` com workers e batching
  - Samplers e shuffling

- [ ] **14-transforms** (`content/14-transforms/`)
  - `torchvision.transforms`
  - Compose e transforms customizados
  - Data augmentation

- [ ] **15-metrics-validation** (`content/15-metrics-validation/`)
  - M√©tricas de classifica√ß√£o e regress√£o
  - Train/val/test split
  - Cross-validation

**Se√ß√µes da documenta√ß√£o relevantes:**
- `torch.utils.data`
- Buscar: "Dataset", "DataLoader", "transforms"

---

### Bloco 5: Arquiteturas Avan√ßadas (M√≥dulos 16-20) - PRIORIDADE BAIXA

T√≥picos mais avan√ßados para depois.

- [ ] **16-cnns** (`content/16-cnns/`)
  - Arquiteturas de CNNs (LeNet, VGG, ResNet)
  - Pooling e stride
  - Feature maps e receptive field

- [ ] **17-rnns-lstm** (`content/17-rnns-lstm/`)
  - RNN b√°sico
  - LSTM e GRU
  - Bidirectional e stacking

- [ ] **18-attention-transformers** (`content/18-attention-transformers/`)
  - Self-attention
  - Multi-head attention
  - Positional encoding
  - Transformer encoder/decoder

- [ ] **19-transfer-learning** (`content/19-transfer-learning/`)
  - Modelos pr√©-treinados
  - Fine-tuning vs feature extraction
  - Freezing layers

- [ ] **20-deploy** (`content/20-deploy/`)
  - TorchScript e JIT
  - ONNX export
  - Quantiza√ß√£o
  - Serving com TorchServe

---

## üìö Recursos Dispon√≠veis

### Documenta√ß√£o PyTorch
Arquivo completo em: `docs_pytorch/docs.pytorch.org-llms.md`

**Aten√ß√£o:** Este arquivo tem ~160K linhas. Use busca por se√ß√µes espec√≠ficas:
- Grep por fun√ß√£o/classe espec√≠fica (ex: "torch.nn.Linear")
- O arquivo est√° organizado por t√≥picos com headers markdown

### Estrutura de Cada M√≥dulo
```
content/{module-id}/
‚îú‚îÄ‚îÄ lesson.mdx      # Conte√∫do da li√ß√£o (MDX com componentes custom)
‚îî‚îÄ‚îÄ exercises.json  # Exerc√≠cios com starter code, hints, valida√ß√£o
```

### Componentes MDX Dispon√≠veis
- `<CodeCell id="unique-id">` - C√©lula de c√≥digo execut√°vel
- `<Exercise id="ex-id" difficulty="easy|medium|hard">` - Exerc√≠cio interativo
- `<Callout type="info|warning|tip|important|success">` - Caixas de destaque
- `<DocRef to="url">` - Refer√™ncia √† documenta√ß√£o oficial

---

## üéØ Diretrizes de Enriquecimento

### Meta de Expans√£o
- Cada m√≥dulo deve ter 2-3x o conte√∫do original
- ~450-700 linhas por lesson.mdx (dependendo do t√≥pico)
- 5-8 exerc√≠cios por m√≥dulo (progress√£o de dificuldade)

### Padr√£o de Conte√∫do
1. **Se√ß√£o "Por que [t√≥pico]?"** - Motiva√ß√£o e contexto
2. **Conceitos fundamentais** - Explica√ß√£o clara com analogias
3. **C√≥digo execut√°vel** - CodeCells com exemplos comentados
4. **Tabelas de refer√™ncia** - Para consulta r√°pida
5. **Callouts estrat√©gicos** - Dicas, warnings, informa√ß√µes
6. **Exerc√≠cios graduados** - Easy ‚Üí Medium ‚Üí Hard
7. **Resumo final** - Recap dos pontos principais

### Estrutura de Exerc√≠cio (exercises.json)
```json
{
  "ex-unique-id": {
    "starterCode": "import torch\n\n# C√≥digo inicial...\nresult = ",
    "hints": [
      "Primeira dica mais gen√©rica",
      "Segunda dica mais espec√≠fica",
      "Terceira dica praticamente d√° a resposta"
    ],
    "validation": {
      "type": "assert",
      "tests": [
        "assert result == expected, 'Mensagem de erro descritiva'"
      ]
    },
    "solution": "result = torch.something()"
  }
}
```

### Idioma
- Todo conte√∫do em **Portugu√™s (PT-BR)**
- Coment√°rios em c√≥digo em portugu√™s
- Nomes de vari√°veis podem ser em ingl√™s (padr√£o t√©cnico)

---

## ‚ö†Ô∏è Pontos de Aten√ß√£o

1. **N√£o modificar arquivos existentes** dos m√≥dulos 1-4 (j√° enriquecidos)
2. **Verificar sintaxe MDX** - Os componentes custom t√™m sintaxe espec√≠fica
3. **Testar exerc√≠cios** - Validar que as assertions funcionam
4. **Manter consist√™ncia** - Seguir padr√µes estabelecidos nos m√≥dulos 1-4
5. **Refer√™ncias √† documenta√ß√£o** - Usar se√ß√µes espec√≠ficas do arquivo de docs

---

## üîÑ Workflow Sugerido

1. Ler o m√≥dulo atual (lesson.mdx + exercises.json)
2. Consultar se√ß√µes relevantes em `docs_pytorch/docs.pytorch.org-llms.md`
3. Expandir lesson.mdx com novos conte√∫dos e CodeCells
4. Adicionar novos exerc√≠cios ao exercises.json
5. Verificar consist√™ncia e sintaxe
6. Atualizar este documento marcando o m√≥dulo como completo

---

## üìä Estimativa de Esfor√ßo

| Bloco | M√≥dulos | Complexidade | Status |
|-------|---------|--------------|--------|
| ‚úÖ Autograd (5-7) | 3 | Alta | Completo |
| Redes Neurais (8-12) | 5 | Alta | Pendente |
| Dados (13-15) | 3 | M√©dia | Pendente |
| Arquiteturas (16-20) | 5 | Alta | Pendente |

---

*√öltima atualiza√ß√£o: M√≥dulos 1-7 completados (Fundamentos + Autograd)*
*Pr√≥ximo bloco recomendado: Redes Neurais (8-12)*
