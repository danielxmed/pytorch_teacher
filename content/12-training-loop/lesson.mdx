---
title: "Training Loop Completo"
order: 12
prerequisites: ["08-nn-module", "10-activations-loss", "11-optimizers"]
estimatedMinutes: 55
pytorchVersion: "2.2"
---

# Training Loop Completo

Vamos juntar tudo que aprendemos em um loop de treinamento completo e profissional.

## Estrutura Básica

<CodeCell id="basic-loop">
import torch
import torch.nn as nn
import torch.optim as optim

# 1. Dados
torch.manual_seed(42)
X_train = torch.randn(1000, 20)
y_train = (X_train[:, 0] + X_train[:, 1] > 0).long()

# 2. Modelo
model = nn.Sequential(
    nn.Linear(20, 64),
    nn.ReLU(),
    nn.Linear(64, 2)
)

# 3. Loss e Otimizador
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 4. Training Loop
n_epochs = 10
for epoch in range(n_epochs):
    model.train()

    # Forward
    outputs = model(X_train)
    loss = criterion(outputs, y_train)

    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Métricas
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_train).float().mean()

    print(f"Epoch {epoch+1}/{n_epochs} - Loss: {loss:.4f} - Acc: {accuracy:.4f}")
</CodeCell>

## Loop com Validação

<CodeCell id="with-validation">
import torch
import torch.nn as nn
import torch.optim as optim

# Dados
torch.manual_seed(42)
X = torch.randn(1200, 20)
y = (X[:, 0] + X[:, 1] > 0).long()

# Split
X_train, X_val = X[:1000], X[1000:]
y_train, y_val = y[:1000], y[1000:]

# Modelo
model = nn.Sequential(
    nn.Linear(20, 64),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(64, 2)
)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

for epoch in range(10):
    # --- TREINO ---
    model.train()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # --- VALIDAÇÃO ---
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val)
        val_loss = criterion(val_outputs, y_val)
        _, predicted = torch.max(val_outputs, 1)
        val_acc = (predicted == y_val).float().mean()

    print(f"Epoch {epoch+1}: Train Loss={loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}")
</CodeCell>

<Callout type="important">
Sempre use `model.eval()` e `torch.no_grad()` durante validação/inferência!
</Callout>

## Loop com Mini-batches

<CodeCell id="minibatch">
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

# Dados
X = torch.randn(1000, 20)
y = (X.sum(dim=1) > 0).long()

# DataLoader
dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=32, shuffle=True)

# Modelo
model = nn.Sequential(nn.Linear(20, 64), nn.ReLU(), nn.Linear(64, 2))
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

# Treino
for epoch in range(5):
    model.train()
    total_loss = 0

    for batch_X, batch_y in loader:
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(loader)
    print(f"Epoch {epoch+1}: Avg Loss = {avg_loss:.4f}")
</CodeCell>

## Função de Treino Reutilizável

<CodeCell id="train-function">
import torch
import torch.nn as nn

def train_epoch(model, loader, criterion, optimizer, device='cpu'):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for X, y in loader:
        X, y = X.to(device), y.to(device)

        outputs = model(X)
        loss = criterion(outputs, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * X.size(0)
        _, predicted = outputs.max(1)
        total += y.size(0)
        correct += predicted.eq(y).sum().item()

    return total_loss / total, correct / total

def evaluate(model, loader, criterion, device='cpu'):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for X, y in loader:
            X, y = X.to(device), y.to(device)
            outputs = model(X)
            loss = criterion(outputs, y)

            total_loss += loss.item() * X.size(0)
            _, predicted = outputs.max(1)
            total += y.size(0)
            correct += predicted.eq(y).sum().item()

    return total_loss / total, correct / total

print("Funções train_epoch e evaluate definidas!")
</CodeCell>

## Exercícios

<Exercise id="ex-complete-loop" difficulty="hard">
Implemente um loop de treino completo para 3 epochs com validação. Use as variáveis fornecidas.
</Exercise>

## Resumo

Um training loop completo inclui:
1. `model.train()` antes do treino
2. Forward pass → Loss → `zero_grad()` → `backward()` → `step()`
3. `model.eval()` e `torch.no_grad()` para validação
4. Mini-batches com DataLoader
5. Métricas de acompanhamento (loss, accuracy)

Você completou a Seção 3: Redes Neurais!
