---
title: "Deploy: TorchScript, ONNX e Otimização"
order: 20
prerequisites: ["08-nn-module", "12-training-loop"]
estimatedMinutes: 50
pytorchVersion: "2.2"
---

# Deploy: TorchScript, ONNX e Otimização

Aprenda a preparar seus modelos para produção.

## TorchScript

TorchScript converte modelos PyTorch para um formato que pode rodar sem Python.

<CodeCell id="torchscript-trace">
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 32)
        self.fc2 = nn.Linear(32, 5)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

model = SimpleModel()
model.eval()

# Método 1: Tracing (grava operações com exemplo)
example_input = torch.randn(1, 10)
traced_model = torch.jit.trace(model, example_input)

print("Traced model:")
print(traced_model.code)
</CodeCell>

<CodeCell id="torchscript-script">
import torch
import torch.nn as nn

class ModelWithControl(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(10, 5)

    def forward(self, x, use_relu=True):
        x = self.linear(x)
        if use_relu:  # Controle de fluxo
            x = torch.relu(x)
        return x

# Método 2: Scripting (para controle de fluxo)
model = ModelWithControl()
scripted_model = torch.jit.script(model)

print("Scripted model:")
print(scripted_model.code)
</CodeCell>

<Callout type="tip">
Use `trace` para modelos simples sem controle de fluxo dinâmico. Use `script` para modelos com if/for que dependem de inputs.
</Callout>

## Salvando e Carregando TorchScript

<CodeCell id="save-torchscript">
import torch

# Salvar (não precisa das definições de classe!)
# traced_model.save('model.pt')

# Carregar (funciona mesmo sem a classe original)
# loaded = torch.jit.load('model.pt')

# Para este exemplo, vamos simular
print("Para salvar: model.save('model.pt')")
print("Para carregar: torch.jit.load('model.pt')")
</CodeCell>

## Quantização

Reduz tamanho do modelo e acelera inferência.

<CodeCell id="quantization">
import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)
model.eval()

# Quantização dinâmica (mais simples)
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {nn.Linear},  # Camadas para quantizar
    dtype=torch.qint8
)

# Comparar tamanhos (simulado)
original_params = sum(p.numel() * 4 for p in model.parameters())  # float32 = 4 bytes
# Quantizado usa ~1 byte por parâmetro

print(f"Parâmetros originais: {sum(p.numel() for p in model.parameters()):,}")
print(f"Tamanho original (aprox): {original_params / 1024:.1f} KB")
print("Após quantização: ~4x menor")
</CodeCell>

## torch.compile (PyTorch 2.0+)

<CodeCell id="torch-compile">
import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(100, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)

# torch.compile otimiza o modelo automaticamente
# compiled_model = torch.compile(model)

# Em PyTorch 2.0+, isso pode acelerar 2-3x
print("torch.compile disponível no PyTorch 2.0+")
print("Uso: compiled_model = torch.compile(model)")
print("Benefícios: Fusão de operações, otimização de memória")
</CodeCell>

## Checklist de Deploy

<CodeCell id="deploy-checklist">
import torch.nn as nn

def prepare_for_deployment(model):
    """Prepara modelo para produção."""

    # 1. Modo avaliação
    model.eval()

    # 2. Desabilitar gradientes
    for param in model.parameters():
        param.requires_grad = False

    # 3. Mover para device de produção
    # model = model.to('cuda')  # ou 'cpu'

    return model

model = nn.Linear(10, 5)
model = prepare_for_deployment(model)

print("Checklist de deploy:")
print("✓ model.eval()")
print("✓ requires_grad = False")
print("✓ Device correto")
print("✓ TorchScript ou ONNX export")
print("✓ Quantização (se aplicável)")
print("✓ Benchmark de latência")
</CodeCell>

## Benchmark de Inferência

<CodeCell id="benchmark">
import torch
import torch.nn as nn
import time

model = nn.Sequential(
    nn.Linear(784, 512),
    nn.ReLU(),
    nn.Linear(512, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)
model.eval()

x = torch.randn(1, 784)

# Warmup
for _ in range(10):
    _ = model(x)

# Benchmark
n_runs = 100
start = time.time()
with torch.no_grad():
    for _ in range(n_runs):
        _ = model(x)
elapsed = time.time() - start

print(f"Média por inferência: {elapsed/n_runs*1000:.2f} ms")
print(f"Throughput: {n_runs/elapsed:.0f} inferências/segundo")
</CodeCell>

## Exercícios

<Exercise id="ex-torchscript" difficulty="medium">
Converta um modelo simples para TorchScript usando trace e verifique que produz o mesmo output.
</Exercise>

## Resumo

- TorchScript: Export para produção sem Python
  - `trace`: Para modelos sem controle de fluxo dinâmico
  - `script`: Para modelos com if/for
- Quantização: Reduz tamanho ~4x, acelera inferência
- torch.compile: Otimização automática (PyTorch 2.0+)
- Checklist: eval(), no grad, device, export, benchmark

Parabéns! Você completou o PyTorch Academy!
